{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto select gpus: [0]\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | embedding | Sequential        | 10.5 M\n",
      "1 | loss      | TripletMarginLoss | 0     \n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.5 M    Total params\n",
      "41.964    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a28fd2aa3e04b0aba966752568b7491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1927: PossibleUserWarning: The number of training batches (14) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeabf61b6e34d209e5b6d3e2f9e2d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a2809931c545919afaf726ca652e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved. New best score: 0.614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2933a2a1ef1d4460bcc6f05edbf1cda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.005 >= min_delta = 0.002. New best score: 0.620\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 271>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=257'>258</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=258'>259</a>\u001b[0m     \u001b[39m# fast_dev_run=True, # uncomment to debug\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=259'>260</a>\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=266'>267</a>\u001b[0m     auto_scale_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=267'>268</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=269'>270</a>\u001b[0m \u001b[39m# trainer.tune(model)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=270'>271</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_dataloaders\u001b[39m=\u001b[39;49mtrain_loader, val_dataloaders\u001b[39m=\u001b[39;49mval_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=272'>273</a>\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bada-25/u/hehlif/task3/notebooks/nonlinear_embedding.ipynb#ch0000000vscode-remote?line=273'>274</a>\u001b[0m test_triplets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(test_path, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=748'>749</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=749'>750</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=750'>751</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=764'>765</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=765'>766</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=766'>767</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=718'>719</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=719'>720</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=720'>721</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=804'>805</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=805'>806</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=806'>807</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1229'>1230</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1231'>1232</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1233'>1234</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1235'>1236</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1318'>1319</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1319'>1320</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1320'>1321</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1348'>1349</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1349'>1350</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1350'>1351</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py?line=206'>207</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:297\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=293'>294</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=295'>296</a>\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=296'>297</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=297'>298</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py?line=299'>300</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1634\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1631'>1632</a>\u001b[0m         \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1632'>1633</a>\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1633'>1634</a>\u001b[0m                 fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1635'>1636</a>\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1636'>1637</a>\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=1637'>1638</a>\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:308\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=305'>306</a>\u001b[0m monitor_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_candidates(trainer)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=306'>307</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (trainer\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=307'>308</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=308'>309</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_last_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381\u001b[0m, in \u001b[0;36mModelCheckpoint._save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=378'>379</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_monitor_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=379'>380</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=380'>381</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:658\u001b[0m, in \u001b[0;36mModelCheckpoint._save_none_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=655'>656</a>\u001b[0m \u001b[39m# set the best model path before saving because it will be part of the state.\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=656'>657</a>\u001b[0m previous, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_path, filepath\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=657'>658</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(trainer, filepath)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=658'>659</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_top_k \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m previous \u001b[39mand\u001b[39;00m previous \u001b[39m!=\u001b[39m filepath:\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=659'>660</a>\u001b[0m     trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mremove_checkpoint(previous)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:384\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=382'>383</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, filepath: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=383'>384</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49msave_checkpoint(filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_weights_only)\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=385'>386</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_global_step_saved \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py?line=387'>388</a>\u001b[0m     \u001b[39m# notify loggers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2450\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2437'>2438</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_checkpoint\u001b[39m(\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2438'>2439</a>\u001b[0m     \u001b[39mself\u001b[39m, filepath: _PATH, weights_only: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, storage_options: Optional[Any] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2439'>2440</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2440'>2441</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2441'>2442</a>\u001b[0m \u001b[39m    Runs routine to create a checkpoint.\u001b[39;00m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2442'>2443</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2447'>2448</a>\u001b[0m \n\u001b[1;32m   <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2448'>2449</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py?line=2449'>2450</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49msave_checkpoint(filepath, weights_only\u001b[39m=\u001b[39;49mweights_only, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:445\u001b[0m, in \u001b[0;36mCheckpointConnector.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=436'>437</a>\u001b[0m \u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=437'>438</a>\u001b[0m \n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=438'>439</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=441'>442</a>\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=442'>443</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=443'>444</a>\u001b[0m _checkpoint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py?line=444'>445</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msave_checkpoint(_checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:418\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=409'>410</a>\u001b[0m \u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=410'>411</a>\u001b[0m \n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=411'>412</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=414'>415</a>\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=415'>416</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=416'>417</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero:\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py?line=417'>418</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_io\u001b[39m.\u001b[39;49msave_checkpoint(checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py:54\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=50'>51</a>\u001b[0m fs\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=52'>53</a>\u001b[0m     \u001b[39m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[0;32m---> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=53'>54</a>\u001b[0m     atomic_save(checkpoint, path)\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=54'>55</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=55'>56</a>\u001b[0m     \u001b[39m# todo (sean): is this try catch necessary still?\u001b[39;00m\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=56'>57</a>\u001b[0m     \u001b[39m# https://github.com/PyTorchLightning/pytorch-lightning/pull/431\u001b[39;00m\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py?line=57'>58</a>\u001b[0m     key \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mLightningModule\u001b[39m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py:69\u001b[0m, in \u001b[0;36matomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py?line=66'>67</a>\u001b[0m torch\u001b[39m.\u001b[39msave(checkpoint, bytesbuffer)\n\u001b[1;32m     <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py?line=67'>68</a>\u001b[0m \u001b[39mwith\u001b[39;00m fsspec\u001b[39m.\u001b[39mopen(filepath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/cloud_io.py?line=68'>69</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(bytesbuffer\u001b[39m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fsspec/core.py:122\u001b[0m, in \u001b[0;36mOpenFile.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=120'>121</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=121'>122</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fsspec/core.py:155\u001b[0m, in \u001b[0;36mOpenFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=152'>153</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=153'>154</a>\u001b[0m     \u001b[39m\"\"\"Close all encapsulated file objects\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=154'>155</a>\u001b[0m     _close(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfobjects, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fsspec/core.py:221\u001b[0m, in \u001b[0;36m_close\u001b[0;34m(fobjects, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=218'>219</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=219'>220</a>\u001b[0m         f\u001b[39m.\u001b[39mflush()\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=220'>221</a>\u001b[0m     f\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/core.py?line=221'>222</a>\u001b[0m fobjects\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fsspec/implementations/local.py:335\u001b[0m, in \u001b[0;36mLocalFileOpener.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/implementations/local.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///u/hehlif/miniconda3/lib/python3.8/site-packages/fsspec/implementations/local.py?line=334'>335</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mclose()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.linalg import norm\n",
    "from torch.optim import Adam, SGD\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "base = \"../\"  # where the data is located\n",
    "img_dir = base + \"food\"  # where you unzipped food.zip\n",
    "train_path = base + \"train_triplets.txt\"\n",
    "test_path = base + \"test_triplets.txt\"\n",
    "\n",
    "\n",
    "def get_split(val_ratio):\n",
    "    triplets = np.loadtxt(train_path, delimiter=\" \").astype(int)\n",
    "    train_triplets, val_triplets = train_test_split(\n",
    "        triplets, test_size=val_ratio, random_state=489, shuffle=True\n",
    "    )\n",
    "    return train_triplets, val_triplets\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    pkl_path = f\"{name}_features.pkl\"\n",
    "\n",
    "    if name == \"ResNet18\":\n",
    "        backbone = models.resnet18(pretrained=True)\n",
    "    elif name == \"ResNet34\":\n",
    "        backbone = models.resnet34(pretrained=True)\n",
    "    elif name == \"ResNet50\":\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "    elif name == \"ResNet101\":\n",
    "        backbone = models.resnet101(pretrained=True)\n",
    "    elif name == \"ResNet152\":\n",
    "        backbone = models.resnet152(pretrained=True)\n",
    "    elif name == \"ViT_b_16\":\n",
    "        backbone = models.vit_b_16(pretrained=True)\n",
    "    elif name == \"ViT_b_32\":\n",
    "        backbone = models.vit_b_32(pretrained=True)\n",
    "    else:\n",
    "        sys.exit(\"Error: This model is not implemented.\")\n",
    "\n",
    "    if name.startswith(\"ResNet\"):\n",
    "        features_dim = list(backbone.children())[-1].in_features\n",
    "    elif name.startswith(\"ViT\"):\n",
    "        features_dim = list(backbone.heads.children())[0].in_features\n",
    "\n",
    "    if os.path.exists(pkl_path):\n",
    "        # fetch precomputed features from earlier run\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "    else:\n",
    "        # compute features\n",
    "        if name.startswith(\"ResNet\"):\n",
    "            feature_map = nn.Sequential(*list(backbone.children())[:-1])\n",
    "            features_dim = list(backbone.children())[-1].in_features\n",
    "            tfms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((242, 354)),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ]\n",
    "            )  # preprocessing function\n",
    "        elif name.startswith(\"ViT\"):\n",
    "            feature_map = nn.Sequential(*list(backbone.children())[:-1])[1]\n",
    "            features_dim = list(backbone.heads.children())[0].in_features\n",
    "            tfms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ]\n",
    "            )  # preprocessing function\n",
    "\n",
    "        feature_map.cuda().eval()\n",
    "\n",
    "        n_imgs = 10_000\n",
    "        features = torch.empty((n_imgs, features_dim))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(n_imgs)):\n",
    "                path = os.path.join(img_dir, f\"{str(i).rjust(5, '0')}.jpg\")\n",
    "                if name.startswith(\"ResNet\"):\n",
    "                    img = tfms(Image.open(path)).unsqueeze(0).cuda()\n",
    "                    phi = feature_map(img)  # forward pass\n",
    "                elif name.startswith(\"ViT\"):\n",
    "                    img = tfms(Image.open(path)).unsqueeze(0).cuda()\n",
    "                    x = backbone._process_input(img)\n",
    "                    batch_class_token = backbone.class_token.expand(x.shape[0], -1, -1)\n",
    "                    x = torch.cat([batch_class_token, x], dim=1)\n",
    "                    phi = feature_map(x)[:, 0]\n",
    "                features[i] = phi.squeeze()\n",
    "\n",
    "        features = features.cpu().float()\n",
    "\n",
    "        # save features\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(features, f)\n",
    "\n",
    "    return features, features_dim\n",
    "\n",
    "\n",
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, triplets, features):\n",
    "        self.triplets = triplets\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        a, b, c = self.features[triplet]\n",
    "        return a, b, c\n",
    "\n",
    "\n",
    "# this defines our network.\n",
    "# we use a pytorch lightning LightningModule instead of a nn.Module\n",
    "# for its convenient features. Hence why we implement many of the\n",
    "# methods below, they are reserved by lightning and used by the\n",
    "# trainer\n",
    "class SimilarityNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_dim,\n",
    "        margin=5.0,\n",
    "        embedding_dim=1024,\n",
    "        lr=1e-3,\n",
    "        momentum=0.9,\n",
    "        nesterov=True,\n",
    "        weight_decay=1e-3,\n",
    "        batch_size=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(in_features=features_dim, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=2048, out_features=embedding_dim),\n",
    "        )\n",
    "        self.loss = nn.TripletMarginLoss(margin=margin)\n",
    "        self.val_loss = partial(F.triplet_margin_loss, margin=0)\n",
    "\n",
    "    def forward(self, a, b, c):\n",
    "        embedded_a = self.embedding(a)\n",
    "        embedded_b = self.embedding(b)\n",
    "        embedded_c = self.embedding(c)\n",
    "\n",
    "        return embedded_a, embedded_b, embedded_c\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        a, b, c = batch\n",
    "        embedded_a, embedded_b, embedded_c = self(a, b, c)\n",
    "        loss = self.loss(embedded_a, embedded_b, embedded_c)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, idx):\n",
    "        a, b, c = batch\n",
    "        embedded_a, embedded_b, embedded_c = self(a, b, c)\n",
    "        loss = self.val_loss(embedded_a, embedded_b, embedded_c)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        d_ab = norm(embedded_a - embedded_b, axis=-1).squeeze()\n",
    "        d_ac = norm(embedded_a - embedded_c, axis=-1).squeeze()\n",
    "        acc = (d_ab < d_ac).float().mean()\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def predict_step(self, batch, idx):\n",
    "        a, b, c = batch\n",
    "        embedded_a, embedded_b, embedded_c = self(a, b, c)\n",
    "        d_ab = norm(embedded_a - embedded_b, axis=-1).squeeze()\n",
    "        d_ac = norm(embedded_a - embedded_c, axis=-1).squeeze()\n",
    "        pred = (d_ab <= d_ac).int()\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            momentum=self.momentum,\n",
    "            nesterov=self.nesterov,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "        self.log(\"val_acc\", avg_acc)\n",
    "\n",
    "\n",
    "# first we compute the embedded images with a pretrained network\n",
    "\n",
    "backbone = \"ResNet152\"  # choose one of ResNet[18, 34, 50, 101, 152], ViT_[b_16, b_32]\n",
    "features, features_dim = get_features(backbone)\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 4096\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "weight_decay = 1e-3\n",
    "margin = 5.0\n",
    "val_ratio = 0.2\n",
    "batch_size = 4096\n",
    "num_workers = 32\n",
    "\n",
    "train_triplets, val_triplets = get_split(val_ratio)\n",
    "train_dataset = TripletsDataset(train_triplets, features)\n",
    "val_dataset = TripletsDataset(val_triplets, features)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = SimilarityNet(\n",
    "    features_dim=features_dim,\n",
    "    margin=margin,\n",
    "    lr=learning_rate,\n",
    "    momentum=momentum,\n",
    "    nesterov=nesterov,\n",
    "    weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "bar = TQDMProgressBar(refresh_rate=1)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_acc\", mode=\"max\", min_delta=0.002, patience=30, verbose=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # fast_dev_run=True, # uncomment to debug\n",
    "    accelerator=\"gpu\",\n",
    "    devices=torch.cuda.device_count(),\n",
    "    auto_select_gpus=True,\n",
    "    min_epochs=1,\n",
    "    max_epochs=1000,\n",
    "    callbacks=[bar, early_stop],\n",
    "    auto_lr_find=True,\n",
    "    auto_scale_batch_size=False,\n",
    ")\n",
    "\n",
    "# trainer.tune(model)\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# predict\n",
    "test_triplets = np.loadtxt(test_path, delimiter=\" \").astype(int)\n",
    "test_dataset = TripletsDataset(test_triplets, features)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    persistent_workers=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "predictions = trainer.predict(model, test_loader)\n",
    "predictions = torch.cat(predictions).tolist()\n",
    "\n",
    "df_pred = pd.DataFrame(predictions)\n",
    "sub_path = f\"{backbone}.txt\"\n",
    "df_pred.to_csv(f\"../predictions/{sub_path}\", index=False, header=None)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e751ec60a2bd0b39245754ddff26773aef4db6007d14158e17e04fcfd719773a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
