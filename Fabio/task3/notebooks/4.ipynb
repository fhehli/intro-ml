{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type              | Params\n",
      "---------------------------------------------------\n",
      "0 | efficientnet | EfficientNet      | 5.3 M \n",
      "1 | features     | Sequential        | 4.0 M \n",
      "2 | embedding    | Linear            | 1.3 M \n",
      "3 | loss         | TripletMarginLoss | 0     \n",
      "---------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "4.0 M     Non-trainable params\n",
      "6.6 M     Total params\n",
      "26.401    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ba6d4cfe464c2981432afe3883baed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b51e311d84157ab4dcdaee5097b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80b7c2c52914d6bb2443b2672ff496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.438\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "from torch.linalg import norm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "base = \"../\"  # where the data is located\n",
    "img_dir = base + \"food\"  # where you unzipped food.zip\n",
    "train_path = base + \"train_triplets.txt\"\n",
    "test_path = base + \"test_triplets.txt\"\n",
    "train_split_path = base + \"train_split.txt\"\n",
    "val_split_path = base + \"val_split.txt\"\n",
    "\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "tfms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")  # could also try augmentation\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    # get triplets\n",
    "    with open(train_path) as f:\n",
    "        triplets = f.readlines()\n",
    "    triplets = [triplet.strip().split(\" \") for triplet in triplets]\n",
    "\n",
    "    # split into training and validation set\n",
    "    train_triplets, val_triplets = train_test_split(\n",
    "        triplets, test_size=0.1, random_state=489, shuffle=True\n",
    "    )\n",
    "\n",
    "    # write training and validation set to txt (so that DataSet can access it)\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for item in train_triplets:\n",
    "            f.writelines(\" \".join(item) + \"\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for item in val_triplets:\n",
    "            f.writelines(\" \".join(item) + \"\\n\")\n",
    "\n",
    "\n",
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, triplets_path, img_dir, transform=None):\n",
    "        self.triplets_path = triplets_path\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(self.triplets_path) as f:\n",
    "            triplets = f.readlines()\n",
    "        self.triplets = [triplet.strip().split(\" \") for triplet in triplets][:64]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "\n",
    "        paths = (os.path.join(self.img_dir, f\"{i}.jpg\") for i in triplet)\n",
    "        images = (Image.open(path) for path in paths)\n",
    "\n",
    "        if self.transform:\n",
    "            images = (self.transform(image) for image in images)\n",
    "\n",
    "        img_a, img_b, img_c = images\n",
    "\n",
    "        return img_a, img_b, img_c\n",
    "\n",
    "\n",
    "class SimilarityNet(pl.LightningModule):\n",
    "    def __init__(self, train_dataset, val_dataset, lr=1e-3, batch_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        self.features = nn.Sequential(\n",
    "            *list(self.efficientnet.children())[:-1]\n",
    "        )  # this is efficientnet without its classification layers\n",
    "        self.features.requires_grad_(False)  # no fine-tuning\n",
    "        self.embedding = nn.Linear(1280, 1024)\n",
    "\n",
    "        self.loss = nn.TripletMarginLoss(margin=3.0)\n",
    "        self.val_loss = partial(F.triplet_margin_loss, margin=0)\n",
    "\n",
    "    def forward(self, img_a, img_b, img_c):\n",
    "\n",
    "        phi_a = self.features(img_a)\n",
    "        phi_b = self.features(img_b)\n",
    "        phi_c = self.features(img_c)\n",
    "\n",
    "        phi_a = phi_a.view(phi_a.size(0), -1)\n",
    "        phi_b = phi_b.view(phi_b.size(0), -1)\n",
    "        phi_c = phi_c.view(phi_c.size(0), -1)\n",
    "\n",
    "        embedded_a = self.embedding(phi_a)\n",
    "        embedded_b = self.embedding(phi_b)\n",
    "        embedded_c = self.embedding(phi_c)\n",
    "\n",
    "        return embedded_a, embedded_b, embedded_c\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-2)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        img_a, img_b, img_c = batch\n",
    "        embedded_a, embedded_b, embedded_c = self(img_a, img_b, img_c)\n",
    "        loss = self.loss(embedded_a, embedded_b, embedded_c)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args):\n",
    "        img_a, img_b, img_c = batch\n",
    "        embedded_a, embedded_b, embedded_c = self(img_a, img_b, img_c)\n",
    "        loss = self.val_loss(embedded_a, embedded_b, embedded_c)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_data()\n",
    "\n",
    "    train_dataset = TripletsDataset(train_split_path, img_dir, tfms)\n",
    "    val_dataset = TripletsDataset(val_split_path, img_dir, tfms)\n",
    "\n",
    "    model = SimilarityNet(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        lr=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    bar = TQDMProgressBar(refresh_rate=1)\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=0.0, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=AVAIL_GPUS,\n",
    "        min_epochs=1,\n",
    "        max_epochs=1,\n",
    "        callbacks=[bar, early_stop],\n",
    "        auto_lr_find=False,\n",
    "        auto_scale_batch_size=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # predictions\n",
    "    test_dataset = TripletsDataset(test_path, img_dir, tfms)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    y_hat = []\n",
    "    with torch.no_grad():\n",
    "        for img_a, img_b, img_c in test_loader:\n",
    "            embedded_a, embedded_b, embedded_c = model(img_a, img_b, img_c)\n",
    "            d_ab = norm(embedded_a - embedded_b, axis=-1).squeeze()\n",
    "            d_ac = norm(embedded_a - embedded_c, axis=-1).squeeze()\n",
    "            y_hat += list((1 * (d_ab < d_ac)))\n",
    "\n",
    "    y_hat = [int(x) for x in y_hat]\n",
    "    df_pred = pd.DataFrame(y_hat)\n",
    "    df_pred.to_csv(\"submission.txt\", index=False, header=None)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd77a25fb6a60ea2ba875a28c7c94dfd43c3166db76578799a5a67b58aaf5dd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('intro-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
