{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "root = \"Fabio/task3/\"\n",
    "img_dir = root + \"food\"\n",
    "train_triplets_path = root + \"train_triplets.txt\"\n",
    "test_path = root + \"test_triplets.txt\"\n",
    "triplets_path = root + \"triplets.txt\"\n",
    "train_split_path = root + \"train_split.txt\"\n",
    "val_split_path = root + \"val_split.txt\"\n",
    "\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "learning_rate = 0.063\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "tfms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")  # could also try augmentation\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    # get triplets\n",
    "    with open(train_triplets_path) as f:\n",
    "        triplets = f.readlines()\n",
    "    triplets = [triplet.strip().split(\" \") for triplet in triplets]\n",
    "\n",
    "    # for each triplet we create two samples and write to a new txt file:\n",
    "    #  - one for the original triplet; with label 1\n",
    "    #  - one with the second and third image swapped; with label 0\n",
    "    with open(triplets_path, \"w\") as f:\n",
    "        for triplet in triplets:\n",
    "            f.writelines(\" \".join(triplet) + \" \" + str(1) + \"\\n\")\n",
    "            triplet[1], triplet[2] = triplet[2], triplet[1]\n",
    "            f.writelines(\" \".join(triplet) + \" \" + str(0) + \"\\n\")\n",
    "    with open(triplets_path, \"r\") as f:\n",
    "        triplets = f.readlines()\n",
    "    triplets = [triplet.strip().split(\" \") for triplet in triplets]\n",
    "\n",
    "    # split into training and validation set\n",
    "    train_triplets, val_triplets = train_test_split(\n",
    "        triplets, test_size=0.1, random_state=489, shuffle=True\n",
    "    )\n",
    "\n",
    "    # write training and validation set to txt (so that DataSet can access it)\n",
    "    with open(train_split_path, \"w\") as f:\n",
    "        for item in train_triplets:\n",
    "            f.writelines(\" \".join(item) + \"\\n\")\n",
    "    with open(val_split_path, \"w\") as f:\n",
    "        for item in val_triplets:\n",
    "            f.writelines(\" \".join(item) + \"\\n\")\n",
    "\n",
    "\n",
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, triplets_path, img_dir, transform=None):\n",
    "        self.triplets_path = triplets_path\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(self.triplets_path) as f:\n",
    "            triplets = f.readlines()\n",
    "        triplets = [triplet.strip().split(\" \") for triplet in triplets]\n",
    "        self.labels = [int(triplet[-1]) for triplet in triplets]\n",
    "        self.triplets = [triplet[:-1] for triplet in triplets]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        paths = [os.path.join(self.img_dir, f\"{i}.jpg\") for i in triplet]\n",
    "        images = [Image.open(path) for path in paths]\n",
    "\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "\n",
    "        return images, label\n",
    "\n",
    "\n",
    "class TripletsTestset(Dataset):\n",
    "    def __init__(self, triplets_path, img_dir, transform=None):\n",
    "        self.triplets_path = triplets_path\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(self.triplets_path) as f:\n",
    "            triplets = f.readlines()\n",
    "        self.triplets = [triplet.strip().split(\" \") for triplet in triplets]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "\n",
    "        paths = [os.path.join(self.img_dir, f\"{i}.jpg\") for i in triplet]\n",
    "        images = [Image.open(path) for path in paths]\n",
    "\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, train_dataset, val_dataset, lr=1e-3, batch_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        self.efficientnet.requires_grad_(False)  # no fine-tuning\n",
    "        self.embedding = nn.Sequential(\n",
    "            *list(self.efficientnet.children())[:-1]\n",
    "        )  # this is efficientnet without its classification layers\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=3 * 1280, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100, 2),\n",
    "        )  # the classifier we are training\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, images):\n",
    "        img_a, img_b, img_c = images\n",
    "\n",
    "        phi_a = self.embedding(img_a).squeeze()\n",
    "        phi_b = self.embedding(img_b).squeeze()\n",
    "        phi_c = self.embedding(img_c).squeeze()\n",
    "        phi = torch.concat((phi_a, phi_b, phi_c), dim=1)\n",
    "\n",
    "        y_hat = self.classifier(phi)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-3)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_data()\n",
    "\n",
    "    train_dataset = TripletsDataset(train_split_path, img_dir, tfms)\n",
    "    val_dataset = TripletsDataset(val_split_path, img_dir, tfms)\n",
    "\n",
    "    model = Classifier(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        lr=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    bar = TQDMProgressBar(refresh_rate=1)\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=0.0, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=AVAIL_GPUS,\n",
    "        min_epochs=1,\n",
    "        max_epochs=250,\n",
    "        callbacks=[bar, early_stop_callback],\n",
    "        auto_lr_find=False,\n",
    "        auto_scale_batch_size=False,\n",
    "    )\n",
    "    # trainer.tune(model)\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # predictions\n",
    "    test_dataset = TripletsTestset(test_path, img_dir, tfms)\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    output = trainer.predict(model, dataloaders=test_loader)[0]\n",
    "    logits_list = F.softmax(output, dim=1)\n",
    "    y_hat = []\n",
    "    for logits in logits_list:\n",
    "        y_hat.append(1 if logits[0] > 0.5 else 0)\n",
    "\n",
    "    df_pred = pd.DataFrame(y_hat)\n",
    "    df_pred.to_csv(\"submission.txt\", index=False, header=None)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
